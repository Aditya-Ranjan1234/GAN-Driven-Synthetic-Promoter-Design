# How to Run the DNA Sequence Generation Model

This document provides instructions on how to run the DNA sequence generation model using the Gumbel-Softmax GAN implementation.

## Prerequisites

1. Make sure you have all the required dependencies installed:
   ```
   pip install torch numpy matplotlib tqdm scikit-learn
   ```

2. Ensure your DNA sequence data is in FASTA format and located at:
   ```
   data/clean_all_dna_sequences.fasta
   ```

## Running the Model

### Starting Training from Scratch

To start training the model from scratch for 1000 epochs:

```bash
python run_dna_gan.py --fasta_file data/clean_all_dna_sequences.fasta --num_epochs 1000
```

This will:
- Train the model for 1000 epochs
- Save checkpoints every 10 epochs in the `checkpoints` directory
- Save visualization plots in the `images` directory

### Resuming Training

If your training was interrupted, you can resume from the latest checkpoint:

```bash
python resume_training.py
```

This script will:
1. Find the latest checkpoint in the `checkpoints` directory
2. Resume training from that checkpoint
3. Continue for the specified number of epochs (default: 1000)

### Visualizing Training Progress

To visualize the training progress:

```bash
python visualize_training.py
```

This will:
1. Load the latest checkpoint
2. Extract the training history
3. Plot the training metrics (losses, accuracy, GC content, etc.)
4. Save the plot to `images/training_history.png`

## Command-line Arguments

### run_dna_gan.py

- `--fasta_file`: Path to the FASTA file (default: 'data/clean_all_dna_sequences.fasta')
- `--seq_len`: Length of DNA sequences (default: 150)
- `--num_epochs`: Number of epochs (default: 1000)
- `--batch_size`: Batch size (default: 64)
- `--save_interval`: Interval for saving checkpoints (default: 10)
- `--resume`: Path to checkpoint to resume training (default: None)
- `--discriminator_type`: Type of discriminator ('cnn' or 'lstm', default: 'cnn')
- `--no_cuda`: Disable CUDA (default: False)

### resume_training.py

- `--fasta_file`: Path to the FASTA file (default: 'data/clean_all_dna_sequences.fasta')
- `--seq_len`: Length of DNA sequences (default: 150)
- `--num_epochs`: Number of epochs (default: 1000)
- `--batch_size`: Batch size (default: 64)
- `--save_interval`: Interval for saving checkpoints (default: 10)
- `--checkpoint_dir`: Directory containing checkpoints (default: 'checkpoints')
- `--discriminator_type`: Type of discriminator ('cnn' or 'lstm', default: 'cnn')
- `--no_cuda`: Disable CUDA (default: False)

### visualize_training.py

- `--checkpoint_dir`: Directory containing checkpoints (default: 'checkpoints')
- `--save_path`: Path to save the plot (default: 'images/training_history.png')

## Output Files

- **Checkpoints**: Saved in the `checkpoints` directory
  - `checkpoint_epoch_X.pt`: Checkpoint at epoch X
  - `final_model.pt`: Final model after training
  - `training.log`: Training log file

- **Visualizations**: Saved in the `images` directory
  - `training_history.png`: Plot of training metrics

- **Generated Sequences**: Saved in the `checkpoints` directory
  - `generated_sequences.fasta`: DNA sequences generated by the model

## Notes

- The model uses CUDA by default if available. Use the `--no_cuda` flag to disable it.
- Training for 1000 epochs may take a long time. You can monitor the progress in the terminal.
- The model saves checkpoints every 10 epochs by default, so you can safely interrupt training and resume later.
- The model uses Adam optimizers with learning rates of 1e-4 for both generator and discriminator.
- The generator uses an LSTM with 256 hidden units and the Gumbel-Softmax trick for backpropagation.
- The discriminator can be either CNN-based or LSTM-based (default: CNN).
