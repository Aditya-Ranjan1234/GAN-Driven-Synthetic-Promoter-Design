{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff016bab",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. Import Libraries & Setup\n",
    "# -----------------------------\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Data Import and Preprocessing\n",
    "# -----------------------------\n",
    "# For demonstration, we attempt to load a CSV file \"promoters.csv\"\n",
    "# The CSV is expected to have at least a \"sequence\" column.\n",
    "try:\n",
    "    df = pd.read_csv('promoters.csv')\n",
    "except Exception as e:\n",
    "    print(\"Dataset not found. Generating simulated data.\")\n",
    "    # Simulate a small dataset with DNA sequences\n",
    "    sequences = [\"TATAAAACCGG\", \"GGCCAATTTGGC\", \"TATACCGAATTA\", \"CCGTATAGGCCT\"]\n",
    "    df = pd.DataFrame({'sequence_id': range(len(sequences)), 'sequence': sequences})\n",
    "\n",
    "# If dataset is empty, simulate data\n",
    "if df.empty or 'sequence' not in df.columns:\n",
    "    sequences = [\"TATAAAACCGG\", \"GGCCAATTTGGC\", \"TATACCGAATTA\", \"CCGTATAGGCCT\"]\n",
    "    df = pd.DataFrame({'sequence_id': range(len(sequences)), 'sequence': sequences})\n",
    "\n",
    "# Define the DNA alphabet and helper for one-hot encoding\n",
    "ALPHABET = ['A', 'C', 'G', 'T']\n",
    "\n",
    "def one_hot_encode(seq):\n",
    "    mapping = {nuc: i for i, nuc in enumerate(ALPHABET)}\n",
    "    encoded = np.zeros((len(seq), len(ALPHABET)), dtype=np.float32)\n",
    "    for i, nucleotide in enumerate(seq):\n",
    "        if nucleotide in mapping:\n",
    "            encoded[i, mapping[nucleotide]] = 1.0\n",
    "    return encoded\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "max_length = max(df['sequence'].apply(len))\n",
    "def pad_sequence(seq, max_len):\n",
    "    return seq + 'A' * (max_len - len(seq))  # Pad with 'A' (could use any nucleotide)\n",
    "df['padded'] = df['sequence'].apply(lambda s: pad_sequence(s, max_length))\n",
    "\n",
    "# One-hot encode sequences; shape -> (num_samples, max_length, 4)\n",
    "X = np.array([one_hot_encode(seq) for seq in df['padded']])\n",
    "print(\"Dataset shape (samples, seq_length, 4):\", X.shape)\n",
    "\n",
    "# Create a tf.data.Dataset for training\n",
    "batch_size = 16\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(X).shuffle(100).batch(batch_size)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Define GAN Model (Generator & Discriminator)\n",
    "# -----------------------------\n",
    "latent_dim = 100         # Dimensionality of noise vector input\n",
    "seq_length = max_length  # Length of the promoter sequences\n",
    "num_classes = 4          # Number of nucleotides (A, C, G, T)\n",
    "\n",
    "# Generator: maps latent noise to a (seq_length, 4) probability distribution per position.\n",
    "def build_generator():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(128, activation='relu', input_dim=latent_dim),\n",
    "        tf.keras.layers.Dense(seq_length * num_classes, activation='relu'),\n",
    "        tf.keras.layers.Reshape((seq_length, num_classes)),\n",
    "        # Using softmax so that each position forms a categorical distribution.\n",
    "        tf.keras.layers.Activation('softmax')\n",
    "    ], name=\"Generator\")\n",
    "    return model\n",
    "\n",
    "# Discriminator: classifies a (seq_length, 4) sequence as real or fake.\n",
    "def build_discriminator():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(seq_length, num_classes)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ], name=\"Discriminator\")\n",
    "    return model\n",
    "\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Define Losses and Optimizers\n",
    "# -----------------------------\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Training Step Definition\n",
    "# -----------------------------\n",
    "@tf.function\n",
    "def train_step(real_sequences):\n",
    "    batch_size = tf.shape(real_sequences)[0]\n",
    "    noise = tf.random.normal([batch_size, latent_dim])\n",
    "    \n",
    "    # Generate synthetic sequences\n",
    "    generated_sequences = generator(noise, training=True)\n",
    "    \n",
    "    real_labels = tf.ones((batch_size, 1))\n",
    "    fake_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "    # Train the discriminator: maximize probability of correctly classifying real vs fake.\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "        real_output = discriminator(real_sequences, training=True)\n",
    "        fake_output = discriminator(generated_sequences, training=True)\n",
    "        loss_real = cross_entropy(real_labels, real_output)\n",
    "        loss_fake = cross_entropy(fake_labels, fake_output)\n",
    "        disc_loss = loss_real + loss_fake\n",
    "    gradients_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_disc, discriminator.trainable_variables))\n",
    "\n",
    "    # Train the generator: try to fool the discriminator.\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        generated_sequences = generator(noise, training=True)\n",
    "        fake_output = discriminator(generated_sequences, training=True)\n",
    "        gen_loss = cross_entropy(real_labels, fake_output)\n",
    "    gradients_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_gen, generator.trainable_variables))\n",
    "    \n",
    "    return disc_loss, gen_loss\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Training Loop with Checkpoint Saving\n",
    "# -----------------------------\n",
    "EPOCHS = 50\n",
    "checkpoint_generator = \"generator_checkpoint.h5\"\n",
    "checkpoint_discriminator = \"discriminator_checkpoint.h5\"\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"Epoch {epoch}/{EPOCHS}\")\n",
    "    for real_batch in train_dataset:\n",
    "        d_loss, g_loss = train_step(real_batch)\n",
    "    \n",
    "    print(f\"Epoch {epoch} => Discriminator Loss: {d_loss:.4f}, Generator Loss: {g_loss:.4f}\")\n",
    "    \n",
    "    # Save model checkpoints every 10 epochs, overwriting the previous state.\n",
    "    if epoch % 10 == 0:\n",
    "        generator.save(checkpoint_generator, overwrite=True)\n",
    "        discriminator.save(checkpoint_discriminator, overwrite=True)\n",
    "        print(f\"Checkpoint saved at epoch {epoch}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Synthetic Data Generation\n",
    "# -----------------------------\n",
    "def generate_synthetic_sequences(num_samples):\n",
    "    noise = tf.random.normal([num_samples, latent_dim])\n",
    "    generated = generator(noise, training=False)\n",
    "    # Convert probabilistic outputs to discrete indices via argmax.\n",
    "    generated_indices = tf.argmax(generated, axis=-1).numpy()\n",
    "    \n",
    "    # Map indices to nucleotides.\n",
    "    mapping = {0: 'A', 1: 'C', 2: 'G', 3: 'T'}\n",
    "    synthetic_seqs = []\n",
    "    for indices in generated_indices:\n",
    "        seq = ''.join([mapping[i] for i in indices])\n",
    "        synthetic_seqs.append(seq)\n",
    "    return synthetic_seqs\n",
    "\n",
    "# Generate sample synthetic sequences.\n",
    "synthetic_data = generate_synthetic_sequences(5)\n",
    "print(\"Synthetic Sequences Generated:\")\n",
    "for seq in synthetic_data:\n",
    "    print(seq)\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Evaluation: k-mer Distribution Comparison\n",
    "# -----------------------------\n",
    "def get_kmers(sequence, k=3):\n",
    "    return [sequence[i:i+k] for i in range(len(sequence) - k + 1)]\n",
    "\n",
    "# Compute k-mer frequency for real (using padded sequences) and synthetic data.\n",
    "real_sequences = df['padded'].tolist()\n",
    "real_kmer_counts = Counter()\n",
    "for seq in real_sequences:\n",
    "    real_kmer_counts.update(get_kmers(seq, k=3))\n",
    "\n",
    "synthetic_kmer_counts = Counter()\n",
    "for seq in synthetic_data:\n",
    "    synthetic_kmer_counts.update(get_kmers(seq, k=3))\n",
    "\n",
    "print(\"\\nReal Data 3-mer Distribution:\")\n",
    "print(dict(real_kmer_counts))\n",
    "print(\"\\nSynthetic Data 3-mer Distribution:\")\n",
    "print(dict(synthetic_kmer_counts))\n",
    "\n",
    "# Further evaluation could include statistical tests such as Chi-square to compare these distributions.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
